{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "from agent import *\n",
    "from bandit import *\n",
    "from policy import *\n",
    "from tqdm.notebook import tqdm,trange\n",
    "from IPython.utils import io\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_expert(bandit, i):\n",
    "    experts = [KernelUCB(bandit, UCBPolicy(), gamma=0.1)]\n",
    "    #experts = [OracleExpert(bandit, GreedyPolicy())]\n",
    "    return experts[i % len(experts)]\n",
    "\n",
    "def generate_experts(n, bandit):\n",
    "    return [generate_expert(bandit, i) for i in range(n)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_experiment(bandit,agents,experts,seed,n_prior_experiences,expert_spread,average_expert_distance,reset=True):\n",
    "    np.random.seed(seed)\n",
    "    if reset:\n",
    "        bandit.reset()\n",
    "        [e.reset() for e in experts]\n",
    "    for agent in agents:\n",
    "        agent.reset()\n",
    "    \n",
    "    if reset:\n",
    "        agent.prior_play(experts,n_prior_experiences,expert_spread, bandit, average_expert_distance=average_expert_distance)\n",
    "        \n",
    "        # cache future contexts and predictions for efficiency\n",
    "        bandit.cache_contexts(n_trials, seed) \n",
    "        for i, e in (enumerate(experts)):\n",
    "            e.cache_predictions(bandit, n_trials) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "data=[]\n",
    "n_experiments=1\n",
    "K_N_configurations=((32,4),(4,32))\n",
    "n_trials=1000\n",
    "n_prior_experiences=100\n",
    "average_expert_distances = (0,0.25,0.5,0.75,1)\n",
    "\n",
    "expert_configurations = ('homogeneous','heterogeneous')\n",
    "\n",
    "overall_bar = tqdm(total=n_trials*n_experiments*len(average_expert_distances)*len(K_N_configurations)*len(expert_configurations),smoothing=0,desc='Overal progress')\n",
    "for experiment in  range(n_experiments):\n",
    "    for ((n_arms,n_experts),average_expert_distance) in product(K_N_configurations,average_expert_distances):\n",
    "        \n",
    "\n",
    "        bandit = PerlinBandit(n_arms=n_arms,complexity=3)\n",
    " \n",
    "\n",
    "        experts = generate_experts(n_experts, bandit)\n",
    "        for expert_conf in expert_configurations:\n",
    "            \n",
    "            \n",
    "            #set up agents\n",
    "            agents=[]\n",
    "            agents += [Collective(bandit, GreedyPolicy(),n_experts,expert_spread = expert_conf)]\n",
    "            \n",
    "            #experts doubled for inversion trick\n",
    "            agents += [MAB(bandit, GreedyPolicy(),n_experts*2,expert_spread = expert_conf)]\n",
    "            agents += [Exp4(bandit, Exp3Policy(),n_experts*2,expert_spread = expert_conf,gamma=.5*(2*np.log(n_experts*2 + 1)/(n_arms * n_trials)) ** (1 / 2))]\n",
    "            \n",
    "            \n",
    "            expected_regret = 8*(1+np.log(n_trials))\n",
    "            sqcb_gamma = 100*(n_arms*n_trials/(expected_regret))**(1/2)\n",
    "            agents += [SquareCB(bandit, SCBPolicy(sqcb_gamma),n_experts*2,expert_spread = expert_conf)]\n",
    "            \n",
    "            agents += [LinUCB(bandit, GreedyPolicy(),n_experts,expert_spread = expert_conf)]\n",
    "            \n",
    "            overall_bar.set_description(f\"Initializing experiment {experiment} N:{n_experts}, K:{n_arms}, Δ:{average_expert_distance}, {expert_conf} experts\")\n",
    "        \n",
    "            # set up experiment (initializes bandits and experts)\n",
    "            initialize_experiment(bandit,agents,experts,experiment,n_prior_experiences,expert_conf,average_expert_distance,reset=True)\n",
    "\n",
    "            overall_bar.set_description(f\"Simulating experiment {experiment} N:{n_experts}, K:{n_arms}, Δ:{average_expert_distance}, {expert_conf} experts\")\n",
    "        \n",
    "            # run experiment\n",
    "            results = np.zeros((n_experts+len(agents)+2,n_trials))\n",
    "            for t in range(n_trials):\n",
    "                # Get current context and expert advice\n",
    "                context = bandit.observe_contexts(cache_index=t)\n",
    "                sampled_rewards = bandit.sample(cache_index=t)\n",
    "                advice = np.array([e.value_estimates(cache_index = t,return_std=True) for e in experts])[:,0]\n",
    "                \n",
    "                # Choose action, log reward, and update each agent\n",
    "                meta_context = {'advice':advice,'base':context}\n",
    "                expanded_meta_context = {'advice':np.vstack([advice,1-advice]),'base':context}\n",
    "                for n,agent in enumerate(agents):\n",
    "                    if type(agent) in (MAB,Exp4,SquareCB): #invert trick\n",
    "                        action = agent.choose(expanded_meta_context)\n",
    "                    else:\n",
    "                        action = agent.choose(meta_context)\n",
    "                        \n",
    "                    reward = sampled_rewards[action]\n",
    "                    results[n_experts+n,t] = reward\n",
    "                    agent.observe(reward, action)\n",
    "                    \n",
    "\n",
    "                # Log expert performance\n",
    "                for e, expert in enumerate(experts):\n",
    "                    choice = np.argmax(advice[e]) \n",
    "                    results[e,t] = sampled_rewards[choice]\n",
    "                    \n",
    "                results[-1,t]=np.max(bandit.action_values) #Best expected reward\n",
    "                \n",
    "                results[-2,t]=np.mean(bandit.action_values) #Random policy\n",
    "                \n",
    "                overall_bar.update()\n",
    "            \n",
    "            # log results\n",
    "            for n,agent in enumerate(agents):\n",
    "                agent_score = np.mean(results[n_experts+n])\n",
    "                data.append([agent.short_name(),experiment,agent_score,\"value\",n_arms,n_experts,(n_arms,n_experts),expert_conf,average_expert_distance ])\n",
    "            \n",
    "            for sort_n,n in enumerate(sorted(range(n_experts),key=lambda n: np.mean(results[n]))):\n",
    "                data.append([f\"expert {sort_n}\",experiment,np.mean(results[n]),\"value\",n_arms,n_experts,(n_arms,n_experts),expert_conf,average_expert_distance ])\n",
    "            \n",
    "            data.append([f\"random\",experiment,np.mean(results[-2]),\"value\",n_arms,n_experts,(n_arms,n_experts),expert_conf,average_expert_distance ])\n",
    "            data.append([f\"optimal\",experiment,np.mean(results[-1]),\"value\",n_arms,n_experts,(n_arms,n_experts),expert_conf,average_expert_distance ])\n",
    "        \n",
    "            \n",
    "overall_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance plots\n",
    "We plot performance in function of performance below. In addition to the aggregation algorithms, the average performance of each expert is marked by a gray line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.DataFrame(data,columns=[\"algorithm\",\"seed\",\"average reward\",\"advice type\",\"K\",\"N\",\"(K,N)\",\"configuration\",\"Δ\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette={'Average':'C1','EXP4-IX':'C0','Meta-CMAB':'C2','Meta-MAB':'C3','optimal':'grey','random':'red','SquareCB':'C9'}\n",
    "g = sns.catplot(data=df[(~df.algorithm.str.contains(\"expert\"))&(~df.algorithm.str.contains(\"optimal\"))&(~df.algorithm.str.contains(\"random\"))],y=\"average reward\",x=\"Δ\",hue=\"algorithm\",palette=palette,kind='point',row=\"configuration\",col_order=K_N_configurations,col='(K,N)',markers='.')\n",
    "\n",
    "for ax_rows,row_name in zip(g.axes,expert_configurations):\n",
    "    for ax,col_name in zip(ax_rows,K_N_configurations):\n",
    "        subdf = df[(df['(K,N)']==col_name)&(df.algorithm.str.contains(\"expert\"))&(df.configuration==row_name)].groupby(['algorithm','Δ']).mean()['average reward'].unstack().transpose()\n",
    "        ax.plot(np.array(subdf.index)*(len(average_expert_distances)-1),subdf.values,c='grey',zorder=0,alpha=0.5)\n",
    "        ax.axhline(df[(df['(K,N)']==col_name)&(df.algorithm.str.contains(\"optimal\"))&(df.configuration==row_name)]['average reward'].mean(),zorder=0)\n",
    "        ax.axhline(df[(df['(K,N)']==col_name)&(df.algorithm.str.contains(\"random\"))&(df.configuration==row_name)]['average reward'].mean(),zorder=0,c='red',linestyle='--')\n",
    "        \n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ca83bce31d79961724d3a99884ed190d8747181fead55d10459e88de68786ce"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
